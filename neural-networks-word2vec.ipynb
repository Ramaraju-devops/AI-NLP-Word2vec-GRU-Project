{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949b2795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m digits\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "\n",
    "# Gensim for loading Word2Vec binary\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b371a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Global hyperparameters\n",
    "# -----------------------------\n",
    "DATASET_CSV = os.path.join(\"./datasets\", \"IMDB-Dataset.csv\")\n",
    "W2V_BIN = os.path.join(\"./datasets/word2vec\", \"GoogleNews-vectors-negative300.bin\")\n",
    "\n",
    "MAX_LEN = 200             # Max tokens per review (pad/truncate)\n",
    "VOCAB_SIZE = 50000        # Limit vocabulary size (most frequent words)\n",
    "EMBEDDING_DIM = 300       # GoogleNews vectors are 300-dim\n",
    "EMBEDDING_TRAINABLE = False\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39d6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset shape: (25000, 2)\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 1: Load dataset\n",
    "# =============================================\n",
    "\n",
    "def read_data(file_path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset CSV not found at: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    expected_cols = {\"review\", \"sentiment\"}\n",
    "    if not expected_cols.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"Dataset must contain columns {expected_cols}, found: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df = read_data(DATASET_CSV)\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18756de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned review:\n",
      " one of the other reviewers has mentioned that after watching just oz episode youll be hooked . they are right , as this is exactly what happened with me . br br the first thing that struck me about oz was its brutality and unflinching scenes of violence , which set in right from the word go . trust  ...\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 2: Preprocess text\n",
    "# =============================================\n",
    "\n",
    "def clean_text_series(text_series: pd.Series) -> pd.Series:\n",
    "    # Lowercase\n",
    "    text_series = text_series.astype(str).str.lower()\n",
    "\n",
    "    # Space around punctuation and normalize spaces\n",
    "    text_series = text_series.apply(lambda x: re.sub(r\"([?.!,¿])\", r\" \\1 \", x))\n",
    "    text_series = text_series.apply(lambda x: re.sub(r\"[\\\"\\']\", \"\", x))  # remove straight quotes\n",
    "\n",
    "    # Keep letters and selected punctuation\n",
    "    text_series = text_series.apply(lambda x: re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", x))\n",
    "\n",
    "    # Remove digits\n",
    "    rm_digits = str.maketrans('', '', digits)\n",
    "    text_series = text_series.apply(lambda x: x.translate(rm_digits))\n",
    "\n",
    "    # Strip and reduce multiple spaces\n",
    "    text_series = text_series.str.strip()\n",
    "    text_series = text_series.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "    return text_series\n",
    "\n",
    "\n",
    "df[\"review\"] = clean_text_series(df[\"review\"])\n",
    "print(\"Sample cleaned review:\\n\", df[\"review\"].iloc[0][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3363fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 3: Encode sentiment labels to 0/1\n",
    "# =============================================\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "df[\"sentiment\"] = lb.fit_transform(df[\"sentiment\"])  # positive=1, negative=0\n",
    "print(\"Label classes:\", getattr(lb, 'classes_', None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbd11e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (effective): 50000\n",
      "X shape: (25000, 200) y shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 4: Tokenization and Padding\n",
    "# =============================================\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, lower=True, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"review\"].tolist())\n",
    "\n",
    "# Convert to sequences, then pad\n",
    "sequences = tokenizer.texts_to_sequences(df[\"review\"].tolist())\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "y = df[\"sentiment\"].to_numpy().astype(np.int32)\n",
    "\n",
    "word_index = tokenizer.word_index  # dict: token -> index\n",
    "vocab_size_effective = min(VOCAB_SIZE, len(word_index) + 1)  # +1 for padding idx 0\n",
    "print(\"Vocabulary size (effective):\", vocab_size_effective)\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4ff589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec KeyedVectors (this may take a minute)...\n",
      "Word2Vec loaded. Vocab size: 3000000\n",
      "Embedding matrix shape: (50000, 300) | OOV tokens (within cap): 12536\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 5: Load pre-trained Word2Vec vectors (GoogleNews) and build embedding matrix\n",
    "# =============================================\n",
    "\n",
    "if not os.path.exists(W2V_BIN):\n",
    "    raise FileNotFoundError(f\"Word2Vec binary not found at: {W2V_BIN}\")\n",
    "\n",
    "print(\"Loading Word2Vec KeyedVectors (this may take a minute)...\")\n",
    "w2v = KeyedVectors.load_word2vec_format(W2V_BIN, binary=True)\n",
    "print(\"Word2Vec loaded. Vocab size:\", len(w2v.key_to_index))\n",
    "\n",
    "# Build embedding matrix for our tokenizer vocab\n",
    "embedding_matrix = np.zeros((vocab_size_effective, EMBEDDING_DIM), dtype=np.float32)\n",
    "not_found = 0\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= vocab_size_effective:\n",
    "        continue\n",
    "    if word in w2v.key_to_index:\n",
    "        embedding_matrix[idx] = w2v[word]\n",
    "    else:\n",
    "        not_found += 1\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape} | OOV tokens (within cap): {not_found}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6418c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (20000, 200) (20000,)\n",
      "Test shapes: (5000, 200) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 6: Train-test split\n",
    "# =============================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d16f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My-Learning-AI-2025\\AI-Learning-tcs\\Machine-Learning\\ramenv310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"w2v_gru_sentiment\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"w2v_gru_sentiment\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">15,000,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ pretrained_embedding            │ ?                      │    \u001b[38;5;34m15,000,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)                     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,000,000</span> (57.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,000,000\u001b[0m (57.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,000,000</span> (57.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m15,000,000\u001b[0m (57.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 7: Build GRU model\n",
    "# =============================================\n",
    "\n",
    "def build_model(vocab_size: int, embedding_dim: int, embedding_matrix: np.ndarray) -> tf.keras.Model:\n",
    "    model = Sequential(name=\"w2v_gru_sentiment\")\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=MAX_LEN,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=EMBEDDING_TRAINABLE,\n",
    "            name=\"pretrained_embedding\",\n",
    "        )\n",
    "    )\n",
    "    model.add(GRU(128, name=\"gru\"))\n",
    "    model.add(Dense(128, activation=\"relu\", name=\"dense_hidden\"))\n",
    "    model.add(Dropout(0.3, name=\"dropout\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size_effective, EMBEDDING_DIM, embedding_matrix)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5bc3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 345ms/step - accuracy: 0.5289 - loss: 0.6903 - val_accuracy: 0.5205 - val_loss: 0.6869\n",
      "Epoch 2/3\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 274ms/step - accuracy: 0.5482 - loss: 0.6793 - val_accuracy: 0.5080 - val_loss: 0.6920\n",
      "Epoch 3/3\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 291ms/step - accuracy: 0.5587 - loss: 0.6762 - val_accuracy: 0.7335 - val_loss: 0.5836\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 8: Train\n",
    "# =============================================\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f220b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.7246 - loss: 0.5856\n",
      "\n",
      "Test Accuracy (Keras evaluate): 72.46% | Loss: 0.5856\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step\n",
      "Test Accuracy (sklearn manual): 72.46%\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.63      0.70      2505\n",
      "    positive       0.69      0.82      0.75      2495\n",
      "\n",
      "    accuracy                           0.72      5000\n",
      "   macro avg       0.73      0.72      0.72      5000\n",
      "weighted avg       0.73      0.72      0.72      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 9: Evaluate\n",
    "# =============================================\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest Accuracy (Keras evaluate): {acc*100:.2f}% | Loss: {loss:.4f}\")\n",
    "\n",
    "# Manual accuracy for sanity check\n",
    "y_pred_prob = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "y_pred_label = (y_pred_prob >= 0.5).astype(int)\n",
    "manual_acc = accuracy_score(y_test, y_pred_label)\n",
    "print(f\"Test Accuracy (sklearn manual): {manual_acc*100:.2f}%\")\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_label, target_names=[\"negative\", \"positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0095fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\n",
      "Sample predictions:\n",
      "- Text: This movie was absolutely fantastic! The performances were stunning and the stor...\n",
      "  Prob(positive)=0.576 -> Pred=positive\n",
      "- Text: Terrible. I wasted two hours of my life. The plot was dull and the acting was wo...\n",
      "  Prob(positive)=0.576 -> Pred=positive\n",
      "- Text: Not bad, but it could have been better. Some parts were enjoyable though....\n",
      "  Prob(positive)=0.576 -> Pred=positive\n",
      "- Text: A masterpiece that will be remembered for years!...\n",
      "  Prob(positive)=0.576 -> Pred=positive\n",
      "- Text: I wouldn't recommend this to anyone....\n",
      "  Prob(positive)=0.576 -> Pred=positive\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 10: Sample predictions on unseen text\n",
    "# =============================================\n",
    "\n",
    "sample_texts = [\n",
    "    \"This movie was absolutely fantastic! The performances were stunning and the story was gripping.\",\n",
    "    \"Terrible. I wasted two hours of my life. The plot was dull and the acting was worse.\",\n",
    "    \"Not bad, but it could have been better. Some parts were enjoyable though.\",\n",
    "    \"A masterpiece that will be remembered for years!\",\n",
    "    \"I wouldn't recommend this to anyone.\"\n",
    "]\n",
    "\n",
    "# Preprocess -> tokenize -> pad\n",
    "sample_clean = clean_text_series(pd.Series(sample_texts)).tolist()\n",
    "sample_seq = tokenizer.texts_to_sequences(sample_clean)\n",
    "sample_pad = pad_sequences(sample_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "sample_probs = model.predict(sample_pad)\n",
    "sample_labels = (sample_probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "for txt, prob, lab in zip(sample_texts, sample_probs.flatten(), sample_labels):\n",
    "    pred = \"positive\" if lab == 1 else \"negative\"\n",
    "    print(f\"- Text: {txt[:80]}...\\n  Prob(positive)={prob:.3f} -> Pred={pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ramenv31011",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
